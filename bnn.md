# Bayesian Neural Networks

## Week 3 (8 Feb - 14 Feb)

- [BNN: a tutorial](https://wjmaddox.github.io/assets/BNN_tutorial_CILVR.pdf) from NYU
- [The case for Bayesian Deep Learning](https://cims.nyu.edu/~andrewgw/caseforbdl.pdf) by Andrew Gordon Wilson

## Week 2 (1 Feb - 7 Feb)

- [NIPS2020 tutorial on Uncertainty Estimation in Deep Learning](https://slideslive.com/38935801/practical-uncertainty-estimation-outofdistribution-robustness-in-deep-learning)

## Week 1 (24 Jan - 31 Jan)

### Objectives

- [ ] Read and underestand the paper [Scalable Bayesian neural networks by layer-wise input augmentation](https://arxiv.org/abs/2010.13498) (iBNN)
  - I underestood the main idea of the paper (the section "Layer-wise input priors"). However, I need more time to undrestand the details (the sections "Variational inference", "Variational ensemble posterior", "Uncertainty decomposition").
- [ ] Implement iBNN and test it on FashionMNIST.
  - I cloned the ibnn repo and trained VGG on Cifar10.

### What I've learned or done
- Veriational inference:
  - [Princeton lecture note](https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf)
- [Bayesian Mixture Models and the Gibbs Sampler](http://www.cs.columbia.edu/~blei/fogm/2015F/notes/mixtures-and-gibbs.pdf)
- [An article on Stochastic Weight Averaging (SWA)](https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-averaging/)
