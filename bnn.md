# Bayesian Neural Networks

## Week 2 (1 Feb - 7 Feb)

## Week 1 (24 Jan - 31 Jan)

### Objectives

- [ ] Read and underestand the paper [Scalable Bayesian neural networks by layer-wise input augmentation](https://arxiv.org/abs/2010.13498) (iBNN)
  - I underestood the main idea of the paper (the section "Layer-wise input priors"). However, I need more time to undrestand the details (the sections "Variational inference", "Variational ensemble posterior", "Uncertainty decomposition").
- [ ] Implement iBNN and test it on FashionMNIST.
  - I cloned the ibnn repo and trained VGG on Cifar10.

### What I've learned or done
- Veriational inference:
  - [Princeton lecture note](https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf)
- [Bayesian Mixture Models and the Gibbs Sampler](http://www.cs.columbia.edu/~blei/fogm/2015F/notes/mixtures-and-gibbs.pdf)
- [An article on Stochastic Weight Averaging (SWA)](https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-averaging/)
